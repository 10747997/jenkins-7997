migrating server from one region to another
allow httpd port no
yum install httpd -y
cd /var/www//html/
echo "hello" > index.html
cd
systemctl start httpd
systemctl status httpd
systemctl enable httpd
yum install vsftpd -y
yum install cifs-utils -y
yum install nfs-utils -y
yum install tree -y
create image from this instance 
copy image and migrate to another region
create instance from that image in that region
now check the packages installed in this region there by rpmquery
you can find packages installed


creating external volume and attaching it to ec2 instance
create volune of  gp2 of any size
attach the volume to instance of same region
lsblk to see block devices
df -h 
mkfs.ext4 /dev/xvdb
mkdir /data
mount /dev/xvdb /data
cd /data
touch training.txt
cd
blkid to get block id copy that id
vim /etc/fstab
click insert and add UUID=copied id /data ext4 defaults 0 0
wq! to exit
df -h
mount -a /data
external volume is mounted successfully


to migrate data from one region to another
create volune of  gp2 of any size
attach the volume to instance of same region
lsblk to see block devices
df -h 
mkfs.ext4 /dev/xvdb
mkdir /data
mount /dev/xvdb /data
cd /data
touch training.txt
cd
blkid to get block id copy that id
vim /etc/fstab
click insert and add UUID=copied id /data ext4 defaults 0 0
wq! to exit
df -h
mount -a /data
external volume is mounted successfully
create snapshot from this volume
copy this snapshot to another region
make volume from this snapshot and atatch to instance in that region
just mount on that region
then you can see the data from other region


efs creation
create efs filesystem 
and select your security group in place of default
now make 3 instances linux,ubuntu,readhat
enable nfs in inbound rules
select different subnet region
on linux instance
rpmquery nfs-utils
no need to install 
mkdir /linux
copy mount via ip address and replace efs with /linux in end
cd /linux
touch a.txt{1..10}
on redhat instance
rpmquery nfs-utils
yum install nfs-utils -y
mkdir /redhat
copy mount via ip address and replace efs with /redhat in end
cd /redhat
touch b.txt{1..10}
on ubuntu instance
apt update
apt install nfs-common
mkdir /ubuntu
copy mount via ip address and replace efs with /ubuntu in end
cd /ubuntu
touch c.txt{1..10}
if to replicate
create another file system in another region ad create replication in this region
on instance
mkdir /name
copy mount via ip address and replace efs with /nithin in end
cd /nithin
ll
now you can see all files created on other side


vpc
create vpc 10.0.0.0/16
subnets public 10.0.0.0/24
        private 10.0.1.0/24
routetables public
            private
create internet gateway and attach to vpc
create nat gateway select public subnet
in public route add internet gateway route and in subnet config enable public subnet
in private route add nat gateway route and in subnet config enable private subnet
create 2 instance one with public subnet and enable public connection and private subnet
enable icmp and httpd rules
yum install httpd -y
cd /var/www/html/
echo "hi" > index.html
vim key.pem
paste key from notepad
chmod 400 key.pem
connect private instance
ping public instance ip address
ping google.com
repeat the same process in another server 
vpc 20.0.0.0/16
subnets public 20.0.0.0/24
        private 20.0.1.0/24
create peering connections
generate ssh-keygen and add public key in each others authorized keys files
scp filename.txt root@ip address:/mnt on one region
scp filename.txt root@ip address:/tmp on other region



for s3 bucket 
yum update all
sudo yum install automake fuse fuse-devel gcc-c++ git libcurl-devel libxml2-devel make openssl-devel
git clone https://github.com/s3fs-fuse/s3fs-fuse.git
cd s3fs-fuse
./autogen.sh
./configure --prefix=/usr --with-openssl
make
sudo make install
which s3fs
touch /etc/passwd-s3fs
vim /etc/passwd-s3fs
Your_accesskey:Your_secretkey
sudo chmod 640 /etc/passwd-s3fs
mkdir /mys3bucket
s3fs your_bucketname -o use_cache=/tmp -o allow_other -o uid=1001 -o mp_umask=002 -o multireq_max=5 /mys3bucket
df -Th
cd /mys3bucket
echo "this is a test file to check s3fs" >> test.txt
ls














Volumes 
lsblk
    2  df -h
    3  mkfs
    4  mkfs.ext4 /dev/xvdb
    5  lsblk
    6  df -h
    7  mkdi
    8  mkdir /data
    9  mount /dev/xvdb/data
   10  mount /dev/xvdb /data
   11  df -h
   12  cd data
   13  cd /dat
   14  cd /data
   15  touch training.txt{1..10}
   16  ls
   17  rm -rf lost+found
   18  ls
   19  blkid
   20  vim /etc/fstab
   21  df -h
   22  mount -a /data
   23  lsblk
   24  ls
   25  cd
   26  cd /data
   27  ls
   28  lsblk
   29  df -h
   30  cd /data
   31  ls
   32  cd
   33  lsblk
   34  df -h
   35  resize2fs /dev/xvdb
   36  df -h
   37  cd /data
   38  ls
   39  lsblk
   40  cd
   41  lsblk
   42  df -h
   43  mkfs.ext4 /dev/xvdc
   44  lsblk
   45  df -h
   46  mkfs.ext4 /dev/xvdc
   47  lsblk
   48  df -h
   49  mkdir /nithindata
   50  mount /dev/xvdc/nithindata
   51  mount /dev/xvdc/nithindata/
   52  mount /dev/xvdc /nithindata
   53  cd /data
   54  ll
   55  mv * / nithindata
   56  mv */nithindata
   57  mv */ nithindata
   58  mv */ /nithindata
   59  mv * /nithindata
   60  cd /nithindata
   61  ll
   62  cd
   63  blkid
   64  vim /etc/fstab
   65  mount -a /nithindata
   66  lsblk
   67  df -h
   68  systemctl daemon-reload
   69  umount /data
   70  df -h
   71  cd /nithindata
   72  ls
   73  lsblk
   74  df -h
   75  lsblk
   76  df -h
   77  cd /nithinreddy
   78  cd /nithindata
   79  ls
   80  cd
   81  lsblk
   82  df -h
   83  xfs_growfs -d /dev/xda 1
   84  xfs_growfs -d /dev/xda1
   85  xfs_growfs -d /dev/xvda1
   86  df -h
   87  growpart /dev/xvda 1
   88  xfs_growfs -d /dev/xvda1
   89  df -h
   90  history


pwd
    2  exit
    3  yum update -y
    4  rpmquery httpd
    5  yum install httpd -y
    6  rpmquery httpd
    7  cd /var/www/html/
    8  ll
    9  cd /var/www/html/
   10  ll
   11  echo "this is my web server" > index.html
   12  ll
   13  cat index.html
   14  cd
   15  systemctl start httpd
   16  systemctl status  httpd
   17  systemctl enable httpd
   18  ip a s
   19  curl http://localhost
   20  rpmquery httpd
   21  lsblk -fs
   22  lsblk
   23  mkfs
   24  mkfs.ext4 /dev/xvdb
   25  blkid
   26  mkdir data
   27  mount/dev/xvdb data
   28  mount /dev/xvdb data
   29  df -h
   30  cd data
   31  touch training.txt{1..100}
   32  ll
   33  cd
   34  history
   35  cd data
   36  ll
   37  blkid
   38  vim /etc/fstab
   39  yum install vim -y
   40  vim /etc/fstab
   41  mount -a
   42  systemctl daemon-reload
   43  cd data
   44  mount -a
   45  cd data
   46  ls
   47  history



cat > nithin.txt
    2  scp nithin.txt root@20.0.0.96:/mnt
    3  cd /tmp
    4  ll
    5  cd /tmp/
    6  ll
    7  ping 20.0.0.96
    8  cd
    9  history
   10  ssh-keygen
   11  cd .ssh/
   12  ls
   13  ll
   14  rm -rf known_hosts
   15  rm -rf known_hosts.old
   16  vim authorized_keys
   17  ll
   18  cat id_rsa.pub
   19  ll
   20  vim authorized_keys
   21  scp nithin.txt root@20.0.0.96:/mnt
   22  cd /tmp
   23  ls
   24  clear
   25  ping 20.0.0.96
   26  cd .ssh
   27  ls
   28  cat authorized_keys
   29  cd
   30  ping 20.0.0.96
   31  cd /tmp
   32  ll
   33  cd /mnt
   34  ls
   35  ll
   36  cd
   37  ssh-keygen
   38  y
   39  cd .ssh
   40  ls
   41  cat authorized_keys
   42  cd
   43  ll
   44  scp nithin.txt root@20.0.0.96:/tmp
   45  cd /mnt
   46  ll
   47  history

.........................................................................................................................................
to install kubernates kubeadm
# Kubeadm Installation Guide

This guide outlines the steps needed to set up a Kubernetes cluster using kubeadm.

## Pre-requisites

- Ubuntu OS (Xenial or later)
- sudo privileges
- Internet access
- t2.medium instance type or higher

---

## AWS Setup

- Make sure your all instance are in same **Security group**.
- Expose port **6443** in the **Security group**, so that worker nodes can join the cluster.

---

## Execute on Both "Master" & "Worker Node"

Run the following commands on both the master and worker nodes to prepare them for kubeadm.


# disable swap
sudo swapoff -a

# Create the .conf file to load the modules at bootup
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

# sysctl params required by setup, params persist across reboots
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

# Apply sysctl params without reboot
sudo sysctl --system

## Install CRIO Runtime
sudo apt-get update -y
sudo apt-get install -y software-properties-common curl apt-transport-https ca-certificates gpg

sudo curl -fsSL https://pkgs.k8s.io/addons:/cri-o:/prerelease:/main/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/cri-o-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/cri-o-apt-keyring.gpg] https://pkgs.k8s.io/addons:/cri-o:/prerelease:/main/deb/ /" | sudo tee /etc/apt/sources.list.d/cri-o.list

sudo apt-get update -y
sudo apt-get install -y cri-o

sudo systemctl daemon-reload
sudo systemctl enable crio --now
sudo systemctl start crio.service

echo "CRI runtime installed successfully"

# Add Kubernetes APT repository and install required packages
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

sudo apt-get update -y
sudo apt-get install -y kubelet="1.29.0-*" kubectl="1.29.0-*" kubeadm="1.29.0-*"
sudo apt-get update -y
sudo apt-get install -y jq

sudo systemctl enable --now kubelet
sudo systemctl start kubelet


---

## Execute ONLY on "Master Node"


sudo kubeadm config images pull

sudo kubeadm init

mkdir -p "$HOME"/.kube
sudo cp -i /etc/kubernetes/admin.conf "$HOME"/.kube/config
sudo chown "$(id -u)":"$(id -g)" "$HOME"/.kube/config


# Network Plugin = calico
kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.0/manifests/calico.yaml

kubeadm token create --print-join-command




## Execute on ALL of your Worker Node's

1. Perform pre-flight checks

   
   sudo kubeadm reset pre-flight checks
   

2. Paste the join command you got from the master node and append `--v=5` at the end.

   
   sudo your-token --v=5
   

   > Use sudo before the token.



## Verify Cluster Connection

**On Master Node:**

```bash
kubectl get nodes


   

---

## Optional: Labeling Nodes

If you want to label worker nodes, you can use the following command:


kubectl label node <node-name> node-role.kubernetes.io/worker=worker

---

$ kubectl completion bash > /etc/profile.d/k8s
$ source /etc/profile.d/k8s
$ echo "source /etc/profile.d/k8s" >> .bashrc


...........................................................................................................................................
kubernates cluster with kops
Setup Kubernetes (K8s) Cluster on AWS
Create Ubuntu EC2 instance

install AWSCLI
apt install unzip -y
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install


Install kubectl
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
kubectl version --client

INstall kops

curl -Lo kops https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '"' -f 4)/kops-linux-amd64
chmod +x ./kops
sudo mv ./kops /usr/local/bin/

Create an IAM user/role with Route53, EC2, IAM and S3 full access

Attach IAM role to ubuntu server

Note: If you create IAM user with programmatic access then provide Access keys.
  aws configure
Install kops on ubuntu instance:

 curl -LO https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '"' -f 4)/kops-linux-amd64
 chmod +x kops-linux-amd64
 sudo mv kops-linux-amd64 /usr/local/bin/kops
 sudo mv kops-linux-amd64 /usr/local/bin/kops /bin/
 
To check Kops Version
  kops version 

Create a Route53 private hosted zone (you can create Public hosted zone if you have a domain)

create an S3 bucket

 aws s3 mb s3://dev.k8s.sanjayskv.in
To Check Bucket: aws s3 ls
Expose environment variable:

 export KOPS_STATE_STORE=s3://dev.k8s.sanjayskv.in
Create sshkeys before creating cluster

 ssh-keygen
Create kubernetes cluster definitions on S3 bucket

 kops create cluster --cloud=aws --zones=ap-southeast-1b --name=dev.k8s.sanjayskv.in --dns-zone=sanjayskv.in --dns private
Create kubernetes cluser

  kops update cluster dev.k8s.sanjayskv.in --yes --admin
Validate your cluster

 kops validate cluster

To list nodes

  kubectl get nodes 
Deploying Nginx container on Kubernetes
Deploying Nginx Container

  kubectl run sample-nginx --image=nginx --replicas=2 --port=80
  kubectl get pods
  kubectl get deployments
Expose the deployment as service. This will create an ELB in front of those 2 containers and allow us to publicly access them:

 kubectl expose deployment sample-nginx --port=80 --type=LoadBalancer
 kubectl get services -o wide
To delete cluster

 kops delete cluster dev.k8s.sanjayskv.in --yes
..........................................................................................................................................



 eks ctl
 host EKS CLUSTER 
 create iam role and attach policy
 ECRfull access,EKSpolicy and IAM full access
 apt-get update -y
 apt install unzip -y
 curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
 unzip awscliv2.zip
 sudo ./aws/install
 aws configure
 Install EKS Tool
 curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
 sudo mv /tmp/eksctl /usr/local/bin
 eksctl version
 Install Kubectl
 curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
 sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl 
 kubectl version --client
 Create EKS Cluster
 eksctl create cluster --name my-cluster --region region-code --version 1.29 --vpc-public-subnets subnet-ExampleID1,subnet-ExampleID2 --without-nodegroup

 ##Create a Node Group
 eksctl create nodegroup \
  --cluster my-cluster \
  --region us-east-2 \
  --name my-node-group \
  --node-ami-family Ubuntu2004 \
  --node-type t2.small \
  --subnet-ids subnet-086ced1a84c94a342,subnet-01695faa5e0e61d97 \
  --nodes 3 \
  --nodes-min 2 \
  --nodes-max 4 \
  --ssh-access \
  --ssh-public-key /root/.ssh/id_rsa.pub




 When You want to delete cluster
 eksctl delete cluster --name my-cluster
        

        





   
